<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>6.4 Parallelization issues</TITLE>
<META NAME="description" CONTENT="6.4 Parallelization issues">
<META NAME="keywords" CONTENT="user_guide">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="user_guide.css">

<LINK REL="previous" HREF="node32.html">
<LINK REL="up" HREF="node29.html">
<LINK REL="next" HREF="node34.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html754"
  HREF="node34.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.gif"></A> 
<A NAME="tex2html750"
  HREF="node29.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.gif"></A> 
<A NAME="tex2html746"
  HREF="node32.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.gif"></A> 
<A NAME="tex2html752"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.gif"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html755"
  HREF="node34.html">7 Troubleshooting</A>
<B> Up:</B> <A NAME="tex2html751"
  HREF="node29.html">6 Performance issues (PWscf)</A>
<B> Previous:</B> <A NAME="tex2html747"
  HREF="node32.html">6.3 File space requirements</A>
   <B>  <A NAME="tex2html753"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H2><A NAME="SECTION00074000000000000000">
6.4 Parallelization issues</A>
</H2>

<P>
pw.x and cp.x can run in principle on any number of processors.
The effectiveness of parallelization is ultimately judged by the 
''scaling'', i.e. how the time needed to perform a job scales
 with the number of processors, and depends upon:

<UL>
<LI>the size and type of the system under study;
</LI>
<LI>the judicious choice of the various levels of parallelization 
(detailed in the "Running on parallel machines" sections);
</LI>
<LI>the availability of fast interprocess communications (or lack thereof).
</LI>
</UL>
Ideally one would like to have linear scaling, i.e. <!-- MATH
 $T_N \sim T_0/N_p$
 -->
<I>T</I><SUB>N</SUB> <IMG
 WIDTH="20" HEIGHT="19" ALIGN="BOTTOM" BORDER="0"
 SRC="img7.gif"
 ALT="$ \sim$"> <I>T</I><SUB>0</SUB>/<I>N</I><SUB>p</SUB>
<tex2html_verbatim_mark> for 
<I>N</I><SUB>p</SUB>
<tex2html_verbatim_mark> processors. In addition, one would like to have linear scaling of
the RAM per processor: <!-- MATH
 $O_N \sim O_0/N_p$
 -->
<I>O</I><SUB>N</SUB> <IMG
 WIDTH="20" HEIGHT="19" ALIGN="BOTTOM" BORDER="0"
 SRC="img7.gif"
 ALT="$ \sim$"> <I>O</I><SUB>0</SUB>/<I>N</I><SUB>p</SUB>
<tex2html_verbatim_mark>, so that large-memory systems
fit into the RAM of each processor.

<P>
As a general rule, image parallelization:

<UL>
<LI>may give good scaling, but the slowest image will determine
the overall performances (''load balancing'' may be a problem);
</LI>
<LI>requires very little communications (suitable for ethernet 
communications);
</LI>
<LI>does not reduce the required memory per processor (unsuitable for 
large-memory jobs).
</LI>
</UL>
Parallelization on k-points:

<UL>
<LI>guarantees (almost) linear scaling if the number of k-points
is a multiple of the number of pools;
</LI>
<LI>requires little communications (suitable for ethernet communications);
</LI>
<LI>does not reduce the required memory per processor (unsuitable for 
large-memory jobs).
</LI>
</UL>
Parallelization on plane-waves:

<UL>
<LI>yields good to very good scaling, especially if the number of processors
in a pool is a divisor of <I>N</I><SUB>3</SUB>
<tex2html_verbatim_mark> and <I>N</I><SUB>r3</SUB>
<tex2html_verbatim_mark> (the dimensions along the z-axis 
of the FFT grids, which may coincide);
</LI>
<LI>requires heavy communications (suitable for Gigabit ethernet up to 
4, 8 CPUs at most, specialized communication hardware needed for 8 or more
processors );
</LI>
<LI>yields almost linear reduction of memory per processor with the number
of processors in the pool.
</LI>
</UL>

<P>
A note on scaling: optimal serial performances are achieved when the data are
as much as possible kept into the cache. As a side effect, plane-wave
parallelization may yield superlinear (better than linear) scaling,
thanks to the increase in serial speed coming from the reduction of data size 
(making it easier for the machine to keep data in the cache).

<P>
For each system there is an optimal range of number of processors on which to 
run the job.  A too large number of processors will yield performance 
degradation. If the size of pools is especially delicate: <I>N</I><SUB>p</SUB>
<tex2html_verbatim_mark> should not 
exceed by much <I>N</I><SUB>3</SUB>
<tex2html_verbatim_mark> and <I>N</I><SUB>r3</SUB>
<tex2html_verbatim_mark>. For large jobs, it is convenient to 
further subdivide a pool of processors into ''task groups''.
When the number of processors exceeds the number of FFT planes, 
data can be redistributed to "task groups" so that each group 
can process several wavefunctions at the same time.

<P>
The optimal number of processors for the ''ortho'' (cp.x) or ''ndiag'' 
(pw.x) parallelization, taking care of linear algebra operations 
involving <I>M</I> <TT>x</TT> <I>M</I>
<tex2html_verbatim_mark> matrices, is automatically chosen by the code.

<P>
Actual parallel performances will also depend a lot on the available software 
(MPI libraries) and on the available communication hardware. For
Beowulf-style machines (clusters of PC) the newest version 1.1 and later of 
the OpenMPI libraries (http://www.openmpi.org/) seems to yield better 
performances than other implementations (info by Kostantin Kudin). 
Note however that you need a decent communication hardware (at least 
Gigabit ethernet) in order to have acceptable performances with 
PW parallelization. Do not expect good scaling with cheap hardware: 
plane-wave calculations are by no means an "embarrassing parallel" problem.

<P>
Also note that multiprocessor motherboards for Intel Pentium CPUs typically 
have just one memory bus for all processors. This dramatically
slows down any code doing massive access to memory (as most codes 
in the Q<SMALL>UANTUM </SMALL>ESPRESSO distribution do) that runs on processors of the same
motherboard.
<HR>
<!--Navigation Panel-->
<A NAME="tex2html754"
  HREF="node34.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.gif"></A> 
<A NAME="tex2html750"
  HREF="node29.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.gif"></A> 
<A NAME="tex2html746"
  HREF="node32.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.gif"></A> 
<A NAME="tex2html752"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.gif"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html755"
  HREF="node34.html">7 Troubleshooting</A>
<B> Up:</B> <A NAME="tex2html751"
  HREF="node29.html">6 Performance issues (PWscf)</A>
<B> Previous:</B> <A NAME="tex2html747"
  HREF="node32.html">6.3 File space requirements</A>
   <B>  <A NAME="tex2html753"
  HREF="node1.html">Contents</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
Paolo Giannozzi
2009-07-19
</ADDRESS>
</BODY>
</HTML>
